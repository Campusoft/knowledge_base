# Varios

Apache Arrow Overview
A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more.
http://arrow.apache.org/overview/


Dremio


Dremio is a next-generation data lake engine that liberates your data with live, interactive queries directly on cloud data lake storage.

Dremio es un motor de lago de datos en la nube que proporciona consultas interactivas sobre almacenes de lagos de datos alojados en la nube. Con Dremio no hay que manejar pipelines de datos para extraer y transformar datos dentro de un  almacén de datos separado para alcanzar un rendimiento predictivo. Dremio crea conjuntos de datos virtuales a partir de los datos ingeridos dentro del lago de datos y proporciona una visión uniforme a los consumidores. Presto popularizó la técnica de separar el almacenamiento de la capa  de computación y Dremio la lleva más lejos mejorando el rendimiento y optimizando los costos operativos.

https://www.dremio.com/



scikit-learn
Machine Learning in Python
https://scikit-learn.org/stable/#


Apache Beam Overview

Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.


Great talks on #DataStreaming and the #Beam magic that lets you write your data pipelines once and run them anywhere! #Spark #Flink #Dataflow 


Varios

Hadoop


Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem


Amundsen:

Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accesible and editable by means of Apache Ranger.

